<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Ziteng Cui</title>
  
  <meta name="author" content="Ziteng Cui">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üëë</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Ziteng Cui (Â¥î Â≠êËó§)</name>
              </p>
              <p> I am a Project Assistant Professor (Áâπ‰ªªÂä©Êïô) at the University of Tokyo, affiliated with the <a href="https://www.mi.t.u-tokyo.ac.jp/en">MIL Lab</a> at <a href="https://www.rcast.u-tokyo.ac.jp/en/">RCAST</a>. I received my Ph.D. from same lab in the University of Tokyo (supervisor: <a href="https://www.mi.t.u-tokyo.ac.jp/harada/">Prof. Tatsuya Harada</a>), M.S. from Shanghai Jiao Tong University, and B.S. from Harbin Institute of Technology.
	      </p>
              <p>
              I mainly work on Vision Robustness & Computational Photography & 3D Computer Vision. My favorite things are DOTA2, hiking, anime (JOJO, One Piece, HunterxHunter ...) and Britpop.
	      </p>
	      <p>
		 I'm always open to academic collaborations, feel free to reach out if you'd like to work together.
	      </p>
	
              <p style="text-align:center">
                <a href="mailto:cui@mi.t.u-tokyo.ac.jp">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.co.id/citations?user=niXIRXgAAAAJ&hl=id">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/cuiziteng/">Github</a> &nbsp/&nbsp
		<a href="https://twitter.com/Empire_Xiao_Yan">Twitter</a> &nbsp/&nbsp
		<a href="https://www.linkedin.com/in/ziteng-cui-08782310a/">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/photo.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/photo.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
	      
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	       <p>
		    ÊòîËÄÖÂ∫ÑÂë®Ê¢¶‰∏∫ËÉ°Ëù∂ÔºåÊ†©Ê†©ÁÑ∂ËÉ°Ëù∂‰πüÔºåËá™ÂñªÈÄÇÂøó‰∏éÔºå‰∏çÁü•Âë®‰πü„ÄÇ‰øÑÁÑ∂ËßâÔºåÂàôËòßËòßÁÑ∂Âë®‰πü„ÄÇ‰∏çÁü•Âë®‰πãÊ¢¶‰∏∫ËÉ°Ëù∂‰∏éÔºüËÉ°Ëù∂‰πãÊ¢¶‰∏∫Âë®‰∏éÔºüÂë®‰∏éËÉ°Ëù∂ÔºåÂàôÂøÖÊúâÂàÜÁü£ÔºåÊ≠§‰πãË∞ìÁâ©Âåñ„ÄÇ -- Â∫ÑÂ≠ê„ÄäÈΩêÁâ©ËÆ∫„Äã
		  </p>  
		<p>
		    ÂèçËÄÖÈÅì‰πãÂä®ÔºõÂº±ËÄÖÈÅì‰πãÁî®„ÄÇÂ§©‰∏ã‰∏áÁâ©Áîü‰∫éÊúâÔºåÊúâÁîü‰∫éÊó†„ÄÇ -- ËÄÅÂ≠ê„ÄäÈÅìÂæ∑Áªè„Äã
		  </p>  
	</tbody></table>


<div style="height:200px; overflow-y:auto; padding:10px; border:1px solid #e0e0e0; border-radius:8px; box-shadow:0 2px 6px rgba(0,0,0,0.05);">

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <tbody>
    <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <heading>News</heading>
        <p>
		‚óè 2025.09 &nbsp Two papers (1 co-first author) <b><i>Dr.RAW</i></b> and <b><i>I2-NeRF</i></b> has been accepted by <b><i>NIPS 2025</i></b>, congrats to the team ‚òÄÔ∏è‚òÄÔ∏è <br>
		‚óè 2025.09 &nbsp I'll give an invited talk at <b><i>BMVC 2025</i></b> <a href="https://supercamerai.github.io/" target="_blank">smart camera workshop</a> üéôÔ∏èüéôÔ∏è <br>
        ‚óè 2025.08 &nbsp One paper <b><i>AR-Talk</i></b> has been accepted by <b><i>Siggraph Asia 2025</i></b>, congrats to the team ‚òÄÔ∏è‚òÄÔ∏è <br>
        ‚óè 2025.06 &nbsp I successfully defended my Ph.D. dissertation, <b><i>Dr. Cui</i></b> now üíêüíê <br>
        ‚óè 2025.04 &nbsp I have been selected to the <a href="https://cvpr.thecvf.com/Conferences/2025/CallForDoctoralConsortium" target="_blank">Doctoral Consortium</a> at <b><i>CVPR 2025</i></b> ‚òÄÔ∏è‚òÄÔ∏è <br>
        ‚óè 2025.02 &nbsp One first author paper <b><i>Luminance-GS</i></b> has been accepted by <b><i>CVPR 2025</i></b> ‚òÄÔ∏è‚òÄÔ∏è <br>
        ‚óè 2024.07 &nbsp One first author paper <b><i>RAW-Adapter</i></b> has been accepted by <b><i>ECCV 2024</i></b> ‚òÄÔ∏è‚òÄÔ∏è <br>
        ‚óè 2023.12 &nbsp One first author paper <b><i>Aleth-NeRF</i></b> has been accepted by <b><i>AAAI 2024</i></b> ‚òÄÔ∏è‚òÄÔ∏è <br>
        ‚óè ... ...
        </p>
      </td>
    </tr>
  </tbody>
</table>

</div>
        
	      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
		    
              <heading>Selected Publications</heading>
              <p>
                I'm now interested in the physics modeling of low-level vision and 3D computer vision, I especially interested in the neural radiance field. "*" means authors contribute equally. 
		      Full publication list please refer to <a href="https://scholar.google.co.id/citations?user=niXIRXgAAAAJ&hl=id">here</a>. 
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	

<tr onmouseout="AAAI26_stop()" onmouseover="AAAI26_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='AAAI26'>
				<img src='images/AAAI26_after.png' width="170", height="140">
				</div>
                <img src='images/AAAI26_before.png' width="170", height="140">
              </div>
              <script type="text/javascript">
                function AAAI26_start() {
                  document.getElementById('AAAI26').style.opacity = "1";
                }

                function AAAI26_stop() {
                  document.getElementById('AAAI26').style.opacity = "0";
                }
                AAAI26_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Emergence of Painting Ability via Recognition-driven Evolution</papertitle>
              </a>
              <br>
              Yi Lin, Lin Gu, <strong>Ziteng Cui</strong>, Shenghan Su, Yumo Hao, Yingtao Tian, Tatsuya Harada, Jianfei Yang.
              <br>
              <em>Arxiv</em>, 2025 &nbsp [<a href="",  target="_blank">github</a>]  
              <br>
		    
	      <a href="">website</a> /
              <a href="https://arxiv.org/pdf/2501.04966">arxiv</a> /	    
	      <a href="">bibtex</a> 	    
              <p></p>
              <p>A recognition-driven evolutionary model that creates abstract yet recognizable paintings with minimal strokes and colors, balancing artistic quality and visual efficiency.</p>
            </td>
          </tr>	
			
		
		<tr onmouseout="CVPR25_stop()" onmouseover="CVPR25_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='CVPR25'><video  width="170", height="140" muted autoplay loop>
      			<source src="images/CVPR25_after.mp4" type="video/mp4">
      		</video></div>
                <img src='images/CVPR25_before.png' width="170", height="140">
              </div>
              <script type="text/javascript">
                function CVPR25_start() {
                  document.getElementById('CVPR25').style.opacity = "1";
                }

                function CVPR25_stop() {
                  document.getElementById('CVPR25').style.opacity = "0";
                }
                CVPR25_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Luminance-GS: Adapting 3D Gaussian Splatting to Challenging Lighting Conditions with View-Adaptive Curve Adjustment</papertitle>
              </a>
              <br>
              <strong>Ziteng Cui</strong>, Xuangeng Chu, Tatsuya Harada.
              <br>
              <em>CVPR</em>, 2025 &nbsp [<a href="https://github.com/cuiziteng/Luminance-GS",  target="_blank">github</a>]  <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/cuiziteng/Luminance-GS">
              <br>
		    
	      <a href="https://cuiziteng.github.io/Luminance_GS_web/">website</a> /
              <a href="https://arxiv.org/abs/2504.01503v2">arxiv</a> /	    
	      <a href="https://raw.githubusercontent.com/cuiziteng/cuiziteng.github.io/master/data/Luminance-GS.bib">bibtex</a> /
	      <a href="https://drive.google.com/file/d/1p0M_HmMLEUl4g9qmkO_QLiGlu1l-19q_/view?usp=sharing">poster</a> 	    
              <p></p>
              <p>A simple multi-view curve adjustment method for novel view synthesis under challenging lighting conditions, including low-light, overexposure, and varying exposure.</p>
            </td>
          </tr>	


<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ARTalk.jpg" alt="ARTalk" width="170", height="140">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2502.20323">
                <papertitle>ARTalk: Speech-Driven 3D Head Animation via Autoregressive Model</papertitle>
              </a>
              <br>
              Xuangeng Chu, Nabarun Goswami, <strong>Ziteng Cui</strong>, Hanqin Wang, Tatsuya Harada.
              <br>
              <em>SIGGRAPH ASIA</em>, 2025 &nbsp [<a href="https://github.com/xg-chu/ARTalk",  target="_blank">github</a>]  <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/xg-chu/ARTalk">
              <br>
	      <a href="https://xg-chu.site/project_artalk/">website</a> /
              <a href="https://arxiv.org/abs/2502.20323">arxiv</a> /
	      <a href="https://raw.githubusercontent.com/cuiziteng/cuiziteng.github.io/master/data/ARTalk.bib">bibtex</a>
              <p></p>
              <p>ARTalk generates realistic 3D head motions (lip sync, blinking, expressions, head poses) from audio in real-time.</p>
            </td>
          </tr>
		
<!-- 	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/robustness.png" alt="RAW-Adapter_J" width="170", height="140">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2503.17027">
                <papertitle>RAW-Adapter: Adapting Pre-trained Visual Model to Camera RAW Images and A Benchmark</papertitle>
              </a>
              <br>
              <strong>Ziteng Cui</strong>, Jianfei Yang, Tatsuya Harada.
              <br>
              <em>Arxiv</em>, 2025 &nbsp [<a href="https://github.com/cuiziteng/ECCV_RAW_Adapter",  target="_blank">github</a>]  <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/cuiziteng/ECCV_RAW_Adapter">
              <br>
              <a href="https://arxiv.org/abs/2503.17027">arxiv</a> /
	      <a href="https://raw.githubusercontent.com/cuiziteng/cuiziteng.github.io/master/data/RAW_Adapter_J.bib">bibtex</a>
              <p></p>
              <p>Extension of RAW-Adapter (ECCV 2024), we additionally propose RAW-Bench, which incorporates 17 types of RAW-based common corruptions.</p>
            </td>
          </tr> -->

		
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/bmvc24.png" alt="BMVC24" width="170", height="140">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://bmva-archive.org.uk/bmvc/2024/papers/Paper_307/paper.pdf">
                <papertitle>Discovering an Image-Adaptive Coordinate System for Photography Processing</papertitle>
              </a>
              <br>
              <strong>Ziteng Cui</strong>, Lin Gu, Tatsuya Harada.
              <br>
              <em>BMVC</em>, 2024 &nbsp 
              <br>
              <a href="https://arxiv.org/abs/2501.06448">arxiv</a> /
	      <a href="https://raw.githubusercontent.com/cuiziteng/cuiziteng.github.io/master/data/BMVC24.bib">bibtex</a> 
              <p></p>
              <p>Instead of just using image-adaptive curves or 3D LUTs, why not design an image-adaptive coordinate system tailored to different photography processing tasks?</p>
            </td>
          </tr>
		
	<tr onmouseout="ra_stop()" onmouseover="ra_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='raw_adapter'>
		<img src='images/RA_After.jpg' width="170", height="140"></div>
                <img src='images/RA_Before.jpg' width="170", height="140">
              </div>
              <script type="text/javascript">
                function ra_start() {
                  document.getElementById('raw_adapter').style.opacity = "1";
                }

                function ra_stop() {
                  document.getElementById('raw_adapter').style.opacity = "0";
                }
                ra_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2408.14802">
                <papertitle>RAW-Adapter: Adapting Pre-trained Visual Model to Camera RAW Images</papertitle>
              </a>
              <br>
              <strong>Ziteng Cui</strong>, Tatsuya Harada.
              <br>
              <em>ECCV</em>, 2024 &nbsp [<a href="https://github.com/cuiziteng/ECCV_RAW_Adapter",  target="_blank">github</a>] <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/cuiziteng/ECCV_RAW_Adapter">
 
              <br>
		    
	      <a href="https://cuiziteng.github.io/RAW_Adapter_web/">website</a> /
              <a href="https://arxiv.org/abs/2408.14802">arxiv</a> /	    
	      <a href="https://raw.githubusercontent.com/cuiziteng/cuiziteng.github.io/master/data/RAW_Adapter.bib">bibtex</a> /
	      <a href="https://drive.google.com/file/d/1B9x_KrnnQghRoC0r77ewtsmPUG1UtEIV/view?usp=sharing">poster</a> 	    
              <p></p>
              <p>We analyze the relationship between camera RAW data and sRGB images pre-trained models, and propose RAW-Adapter for effective RAW data based vision tasks.</p>
            </td>
          </tr>	

		
	<tr onmouseout="aleth_stop()" onmouseover="aleth_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='aleth_image'><video  width="170", height="140" muted autoplay loop>
      			<source src="images/shrub_aleth.mp4" type="video/mp4">
      		</video></div>
                <img src='images/show_aleth.png' width="170", height="140">
              </div>
              <script type="text/javascript">
                function aleth_start() {
                  document.getElementById('aleth_image').style.opacity = "1";
                }

                function aleth_stop() {
                  document.getElementById('aleth_image').style.opacity = "0";
                }
                aleth_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2312.09093.pdf">
                <papertitle>Aleth-NeRF: Illumination Adaptive NeRF with Concealing Field Assumption</papertitle>
              </a>
              <br>
              <strong>Ziteng Cui</strong>,
	      Lin Gu, Xiao Sun, Xianzheng Ma, Yu Qiao, Tatsuya Harada.
              <br>
              <em>AAAI</em>, 2024 &nbsp [<a href="https://github.com/cuiziteng/Aleth-NeRF",  target="_blank">github</a>]  <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/cuiziteng%2FAleth-NeRF
">
              <br>
		    
	      <a href="https://cuiziteng.github.io/Aleth_NeRF_web/">website</a> /
              <a href="https://arxiv.org/abs/2312.09093">arxiv</a> /
	      <a href="https://arxiv.org/abs/2303.05807">arxiv (old version)</a> /	    
	      <a href="https://raw.githubusercontent.com/cuiziteng/cuiziteng.github.io/master/data/Aleth-NeRF.bib">bibtex</a> /
	      <a href="https://drive.google.com/file/d/1ZNwMLwKu6uubUHgW9yKTEyOKul8wWBLf/view?usp=sharing">poster</a> 	    
              <p></p>
              <p>Blight NeRF's volume rendering function with concealing fields, to handle novel view synthesis under low-light conditions and overexposure conditions.</p>
            </td>
          </tr>	

	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/iccv23.png" alt="ICCV23" width="170", height="140">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_MonoDETR_Depth-guided_Transformer_for_Monocular_3D_Object_Detection_ICCV_2023_paper.pdf">
                <papertitle>Monodetr: Depth-guided transformer for monocular 3d object detection</papertitle>
              </a>
              <br>
              Renrui Zhang, Han Qiu, Tai Wang, Ziyu Guo, <strong>Ziteng Cui</strong>, Yu Qiao, Hongsheng Li, Peng Gao
              <br>
              <em>ICCV</em>, 2023 &nbsp [<a href="https://github.com/ZrrSkywalker/MonoDETR",  target="_blank">github</a>]  <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/ZrrSkywalker/MonoDETR">
              <br>
              <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_MonoDETR_Depth-guided_Transformer_for_Monocular_3D_Object_Detection_ICCV_2023_paper.pdf">arxiv</a> /
	      <a href="https://raw.githubusercontent.com/cuiziteng/cuiziteng.github.io/master/data/monodetr.bib">bibtex</a> /
	      <a href="https://github.com/ZrrSkywalker/MonoDETR">code</a> /
	      <a href="">poster</a> 
              <p></p>
              <p>MonoDETR introduces a depth-guided transformer for monocular 3D object detection, enhancing Mono3D with non-local depth cues and achieving SOTA results.</p>
            </td>
          </tr>
		
	  <tr onmouseout="dt_stop()" onmouseover="dt_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='dt_image'><img src='images/BMVC22_result.png' width="170", height="140"></div>
                <img src='images/BMVC22_low.jpg' width="170", height="140">
              </div>
              <script type="text/javascript">
                function dt_start() {
                  document.getElementById('dt_image').style.opacity = "1";
                }

                function dt_stop() {
                  document.getElementById('dt_image').style.opacity = "0";
                }
                dt_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2205.14871.pdf">
                <papertitle>You Only Need 90K Parameters to Adapt Light: A Light Weight Transformer for Image Enhancement and Exposure Correction</papertitle>
              </a>
              <br>
              <strong>Ziteng Cui</strong>,
	      Kunchang Li,
	      Lin Gu, Shenghan Su, Peng Gao, Zhengkai Jiang, Yu Qiao, Tatsuya Harada.
              <br>
              <em>BMVC</em>, 2022 &nbsp [<a href="https://github.com/cuiziteng/Illumination-Adaptive-Transformer",  target="_blank">github</a>]  <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/cuiziteng%2FIllumination-Adaptive-Transformer
"> <a href="https://scholar.google.com/citations?hl=zh-CN&vq=eng_computervisionpatternrecognition&view_op=list_hcore&venue=T_DfB2ikUbwJ.2025",  target="_blank">(‚ú® Most Cited in BMVC 2022)</a>
              <br>
              <a href="https://arxiv.org/abs/2205.14871">arxiv</a> /
	      <a href="https://raw.githubusercontent.com/cuiziteng/cuiziteng.github.io/master/data/IAT.bib">bibtex</a> /
	      <a href="https://huggingface.co/spaces/Andy1621/IAT_enhancement">demo</a> / 
              <a href="https://drive.google.com/file/d/1qY9pODkmRmrHeMoQyhX52loTWqE9qcvP/view?usp=share_link">poster</a> 
              <p></p>
              <p>A super light-weight (only 90k+ parameters) transformer-based network Illumination Adaptive Transformer, for real time image enhancement and exposure correction.</p>
            </td>
          </tr>
	
	  <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ECCV_AERIS.jpg" alt="ECCV_Restoredet" width="170", height="140">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690465.pdf">
                <papertitle>Exploring Resolution and Degradation Clues as Self-supervised Signal for Low Quality Object Detection</papertitle>
              </a>
              <br>
              <strong>Ziteng Cui</strong>,
	      Yingying Zhu,
	      Lin Gu, Guo-Jun Qi, Xiaoxiao Li, Renrui Zhang, Zenghui Zhang, Tatsuya Harada.
              <br>
              <em>ECCV</em>, 2022 &nbsp [<a href="https://github.com/cuiziteng/ECCV_AERIS",  target="_blank">github</a>]  <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/cuiziteng%2FECCV_AERIS

">
              <br>
              <a href="https://arxiv.org/abs/2208.03062">arxiv</a> /
	      <a href="https://raw.githubusercontent.com/cuiziteng/cuiziteng.github.io/master/data/ECCV_AERIS.bib">bibtex</a> /
	      <a href="https://drive.google.com/file/d/1aLkCiXC9f8fKbJS9gCMNXQ_mMyICIXvl/view?usp=share_link">poster</a>
              <p></p>
              <p>Combine detection with self-supervised super-resolution, for robust detection under various degradation conditions (noise, blurry, low-resolution).</p>
            </td>
          </tr>
		
	 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/low_after.png" alt="ICCV_MAET" width="170", height="140">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Cui_Multitask_AET_With_Orthogonal_Tangent_Regularity_for_Dark_Object_Detection_ICCV_2021_paper.pdf">
                <papertitle>Multitask AET with Orthogonal Tangent Regularity for Dark Object Detection</papertitle>
              </a>
              <br>
              <strong>Ziteng Cui</strong>,
	      Guo-Jun Qi,
	      Lin Gu, Shaodi You, Zenghui Zhang, Tatsuya Harada.
              <br>
              <em>ICCV</em>, 2021 &nbsp [<a href="https://github.com/cuiziteng/ICCV_MAET",  target="_blank">github</a>]  <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/cuiziteng%2FICCV_MAET
">
              <br>
              <a href="https://arxiv.org/abs/2205.03346">arxiv</a> /
	      <a href="https://raw.githubusercontent.com/cuiziteng/cuiziteng.github.io/master/data/MAET_ICCV.bib">bibtex</a> /
	      <a href="https://drive.google.com/file/d/14FHLdtGbtoNYx64GaGjPAlXWeU61f8-v/view?usp=sharing">poster</a> 
              <p></p>
              <p>Using Camera-ISP pipeline for low-light image synthetic, then using self-supervised learning to improving the performance of low-light condition object detection.</p>
            </td>
          </tr>
	

        </tbody></table>

				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Misc</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
		
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/web_logo.png" alt="LOGO" width="160" height="160">
            </td>
            <td width="75%" valign="center">
			 <a>BMVC 2025 Doctoral Consortium</a>
              <br>
	      	 <a>CVPR 2025 Doctoral Consortium</a>
              <br>
			 <a>CVPR 2025 Outstanding Reviewer</a>
              <br>
              <a>Shanghai Jiao Tong University National Scholarship</a>
              <br>
              <a>Shanghai Jiao Tong University Excellent Graduate Student</a>
              <br>
            </td>
          </tr>
	
		
	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
	
	<tr>
	<a>Reviewer: CVPR, ICCV, ECCV, TPAMI, IJCV, ICLR, ICML, NIPS, ACM MM, AISTATS, BMVC, ACCV, AAAI, Eurographics, Pacific Graphics, etc.</a>
	</tr>
	
					
					
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                This website is borrow from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
                <br>
                Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
